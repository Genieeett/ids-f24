## Web Scraping

This section was written by Melanie Desroches, a senior majoring in statistics and minoring in computer science.
The goal of this section is to introduce web-scraping so that it can be utilized for data science. This will 
include what web-scraping is, how to web-scrape with Python using examples, and how to web-scrape ethically.

### What is Web-Scraping

As data scientists, we often want to collect data from a variety of sources. In the age of the internet, a lot 
of the data we may want to collect is available on a website. However, this data is often times not available 
in an easily downloadable format. This is where web-scraping becomes valuable. Web-scraping is an automated 
process used to gather data from websites. This allows us to access and collect large amounts of data directly 
from web pages if the information is not avalible for download.

Websites are primarily structured with HTML (Hypertext Markup Language), which organizes and displays content. 
Web scrapers parse through this HTML code to identify and extract relevant information. Therefore, it important 
to have a basic understanding of HTML in order to identify what part of the website you are trying to scrape.
The contents of a web page are broken up and identified by elements. Here are some examples of common elements that are 
important for web-scraping:

- `<body>` : identifies the website body
- `<table>` : identifies a table
- `<tbody>` : identifies the body of the table
- `<tr>` : indentifies the row of a table

### How to Web-Scrape with Python

There are many ways to web-scrape with Python. We will cover the two main packages, Beautiful Soup 
and Selenium.

#### Beautiful Soup

The Beautiful Soup Python Library simplifies the process of parsing and navigating HTML and XML documents, making it easier 
to extract data from websites. Beautiful Soup is ideal for scraping data from static websites. Static websites do not change 
based on user actions or require server-side interactions to update content dynamically. Basically, what you see is what you get.
Static websites tend to be pretty simple so scraping from them is relatively easy.

Beautiful Soup can be installed by running
```
pip install beautifulsoup4
```
in your terminal.

#### Selenium

Selenium is used for web browser automation and dynamic websites. Dynamic sites often use backend programming to 
pull data from a database, customize it, and render it in real time based on user requests. This makes Selenium 
great at performing web-scraping tasks that involve multiple pages or performing actions within those pages.
Because dynamic websites tend to be a bit more complex, you need to use a package like Selenium that is more
equiped for the complex structure.

Selenium can be installed by running 
```
pip install selenium
```
in your terminal. To control a web browser, Selenium also requires a WebDriver. Download the driver that matches your 
browser version and operating system, such as Edge Driver for Microsoft or Chrome Edge for Google.

#### Beautiful Soup vs Selenium

Both Beautiful Soup and Selenium are helpful tools in web-scraping. But they both have their strengths and weaknesses.
Beautiful Soup is lightweight, easy to learn, and perfect for working with static HTML content. However, Beautiful Soup 
is more limited when it comes to dynamic websites, which are much more common nowadays. Selenium is better for interacting 
with dynamic web content that loads JavaScript or requires actions like clicking, scrolling, or filling forms. That said, 
selenium can be slower and more resource-intensive since it opens a browser window to simulate real user actions.

#### A Step-by Step Guide to Web-Scraping

1. Find the website URL with the information you want to select
2. Send an HTTP request to the URL and confirm you have access to the page. Generally, 200-299 means the request has been granted and 
  400-499 means that your request is not allowed.
3. Use the "Inspect" tool in your browser to identify the tags, classes, or elements associated with the data you want to extract. This
  can be done by right-clicking on the web page and pressing select. If you hover your clicker over the different sections of HTML,
  the parts of the website that section is associated with will become highlighted. Use this to find the element that is associated
  with the data that you want to scrape.
4. Use a parsing library like Beautiful Soup or Selenium to process the HTML response. Beautiful Soup requires the use of the 
  requests package in order to send a request. Selenium uses the webdriver to send the request.
5. Clean and store the relevant infomation.

### Examples using NYC Open Data

Since this class has used the NYC Open Data, let's build on this data set in order to get some additional information that is
not already available.

#### Beautiful Soup and NYPD Precincts

Say you want to get the adresses of all of the NYPD Precincts in New York City. This information is available in table format 
on the [NYPD website](https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page). 

```{python}
#| echo: true
import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL of the NYPD precincts page
url = "https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page"

# Send a GET request to the page
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
}
response = requests.get(url, headers=headers)

# Check if the request was successful
print(response.status_code)
if response.status_code != 200:
    print(f"Failed to retrieve page: Status code {response.status_code}")
```

Parsing the Table
```{python}
#| echo: true
# Parse the HTML content
soup = BeautifulSoup(response.text, 'html.parser')

# Find the table with class "rt" which holds the precinct data
table = soup.find("table", {"class": "rt"})
    
# Lists to hold the extracted data
precinct_names = []
addresses = []
    
# Extract each row of the table (each row corresponds to one precinct)
for row in table.find_all("tr"):
  # Find the "Precinct" and "Address" columns by data-label attribute
  precinct_cell = row.find("td", {"data-label": "Precinct"})
  address_cell = row.find("td", {"data-label": "Address"})
        
  # If both cells are found, store their text content
  if precinct_cell and address_cell:
    precinct_names.append(precinct_cell.get_text(strip=True))
    addresses.append(address_cell.get_text(strip=True))
```

Storing the Data

```{python}
#| echo: true
# Create a DataFrame with the extracted data
precincts_df = pd.DataFrame({
  "Precinct": precinct_names,
  "Address": addresses
})

# Display the DataFrame
print(precincts_df)
```

#### Selenium and Weather Data


### A Note on Data Ethics 

#### Why Web-Scraping can be un-ethical

#### Some Tips to Help You Scrape Ethically