## Web Scraping

This section was written by Melanie Desroches, a senior majoring in statistics and minoring in computer science.
The goal of this section is to introduce web-scraping so that it can be utilized for data science. This will 
include what web-scraping is, how to web-scrape with Python using examples, and how to web-scrape ethically.

### What is Web-Scraping

As data scientists, we often want to collect data from a variety of sources. In the age of the internet, a lot 
of the data we may want to collect is available on a website. However, this data is often times not available 
in an easily downloadable format. This is where web-scraping becomes valuable. Web-scraping is an automated 
process used to gather data from websites. This allows us to access and collect large amounts of data directly 
from web pages if the information is not avalible for download.

Websites are primarily structured with HTML (Hypertext Markup Language), which organizes and displays content. 
Web scrapers parse through this HTML code to identify and extract relevant information. Therefore, it important 
to have a basic understanding of HTML in order to identify what part of the website you are trying to scrape.
The contents of a web page are broken up and identified by elements. Here are some examples of common elements that are 
important for web-scraping:

- `<body>` : identifies the website body
- `<table>` : identifies a table
- `<tbody>` : identifies the body of the table
- `<tr>` : indentifies the row of a table

### How to Web-Scrape with Python

There are many ways to web-scrape with Python. We will cover the two main packages, Beautiful Soup 
and Selenium.

#### Beautiful Soup

The Beautiful Soup Python Library simplifies the process of parsing and navigating HTML and XML documents, making it easier 
to extract data from websites. Beautiful Soup is ideal for scraping data from static websites. Static websites do not change 
based on user actions or require server-side interactions to update content dynamically. Basically, what you see is what you get.
Static websites tend to be pretty simple so scraping from them is relatively easy.

Beautiful Soup can be installed by running
```
pip install beautifulsoup4
```
in your terminal.

#### Selenium

Selenium is used for web browser automation and dynamic websites. Dynamic sites often use backend programming to 
pull data from a database, customize it, and render it in real time based on user requests. This makes Selenium 
great at performing web-scraping tasks that involve multiple pages or performing actions within those pages.
Because dynamic websites tend to be a bit more complex, you need to use a package like Selenium that is more
equiped for the complex structure.

Selenium can be installed by running 
```
pip install selenium
```
in your terminal. To control a web browser, Selenium also requires a WebDriver. Download the driver that matches your 
browser version and operating system, such as Edge Driver for Microsoft or Chrome Edge for Google.

#### Beautiful Soup vs Selenium

Both Beautiful Soup and Selenium are helpful tools in web-scraping. But they both have their strengths and weaknesses.
Beautiful Soup is lightweight, easy to learn, and perfect for working with static HTML content. However, Beautiful Soup 
is more limited when it comes to dynamic websites, which are much more common nowadays. Selenium is better for interacting 
with dynamic web content that loads JavaScript or requires actions like clicking, scrolling, or filling forms. That said, 
selenium can be slower and more resource-intensive since it opens a browser window to simulate real user actions.

#### A Step-by Step Guide to Web-Scraping

1. Find the website URL with the information you want to select
2. Send an HTTP request to the URL and confirm you have access to the page. Generally, 200-299 means the request has been granted and 
  400-499 means that your request is not allowed.
3. Use the "Inspect" tool in your browser to identify the tags, classes, or elements associated with the data you want to extract. This
  can be done by right-clicking on the web page and pressing select. If you hover your clicker over the different sections of HTML,
  the parts of the website that section is associated with will become highlighted. Use this to find the element that is associated
  with the data that you want to scrape.
4. Use a parsing library like Beautiful Soup or Selenium to process the HTML response
5. Clean and store the relevant infomation

### Examples using NYC Open Data

#### Beautiful Soup and NYPD Precincts

#### Selenium and Weather Data


### A Note on Data Ethics 

#### Why Web-Scraping can be un-ethical

#### Some Tips to Help You Scrape Ethically