# Reinforcement Learning




## What is Reinforcement Learning?

- Reinforcement learning (RL) is a type of machine learning.
- Deep Q-Networks (DQN) 
- Agents learn by interacting with the environment.
- The goal of it is to maximize cumulative reward over time.
- Reinforcement Learning (RL) VS Supervised Learning (SL)

Supervised Learning (SL) and Reinforcement Learning (RL) share some
similarities in their frameworks, but they differ fundamentally in
how they learn and improve over time. In a supervised learning 
setting, training a neural network, such as learning to play a game,
requires a substantial dataset with labeled examples. This involves
recording the gameplay process, including inputs like key presses
and even eye movements. 

The network learns by observing how certain actions (inputs) lead to
specific outcomes (labels), essentially mimicking human actions to
predict what to do next. However, it only imitates what it's been
taught and does not learn to improve on its own beyond the data it's
given.

In contrast, reinforcement learning does not rely on a predefined
dataset with target labels. Instead, it uses a policy network that
transforms input frames from the game into actions. The training
process in RL starts with a network that has no prior knowledge. It
receives input frames directly from the game environment and must
figure out what actions to take to maximize rewards over time. The
simplest way to train this policy network is through policy
gradients, where the model learns by interacting with the
environment and receiving feedback. 

However, RL has a downside: if the model encounters a failure, it
might incorrectly generalize that a certain strategy is bad, leading
to what's known as the credit assignment problem—it struggles to
properly attribute which actions led to success or failure. This
means the network may become overly cautious, reducing its
exploration of potentially good strategies that initially resulted
in failure.




## What does this note contains

- [Actual usages & Scopes & Limitations](#actual-usages--scopes--limitations)
- [Q-learning](#q-learning)
- [An example of simple models](#grid)
- [Rewards and Penalties](#rewards-and-penalties)




## Actual usages & Scopes & Limitations

### Actual usages

- Gaming and Simulations (AlphaGo)
- Cooperate with Bayesian Optimization
- Robotics and Automation
- Self-driving Cars
- Finance and Trading
- Personalization and Recommendations


### Scopes & Limitations

- **Reinforcement Learning vs. Evolutionary Methods**

Evolutionary methods cannot utilize real-time feedback from actions,
making them less efficient in dynamic environments where immediate
learning is advantageous.

- **Policy Gradient Methods**

Unlike evolutionary methods, policy gradients interact with the
environment to improve performance, allowing more efficient use of
detailed feedback from individual interactions.

- **Misunderstanding of Optimization**

Optimization in RL is about improving performance incrementally, not
guaranteeing the best possible outcome in every scenario.




## Q-Learning


### Q-learning Overview

- Aims to learn the optimal action-value function $( q_*(s, a) )$,
regardless of the policy being followed during learning. 
- The main objective is to approximate this function through a
learning process where the agent interacts with its environment,
receiving rewards and updating its knowledge of state-action pairs.


### Q-learning Update Rule
The core formula of Q-learning is based on the Bellman equation for
the action-value function:

**$$ [Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \right]] $$**


- $( Q(s, a) )$: The current Q-value for taking action $( a )$ in
state $( s )$.

- $( \alpha )$: The learning rate, set to 0.1 in your code, which
controls how much new information overrides the old information.

- $( R )$: The reward received after taking action $( a )$ in state
$( s )$.

- $( \gamma )$: The discount factor, set to 0.9 in your code, which
determines the importance of future rewards.

- $( \max_{a'} Q(s', a') )$: The maximum Q-value for the next state
$( s' )$ across all possible actions $( a' )$. This term represents
the best possible future reward from the next state.

- $( Q(s, a) )$ (the initial term on the right side): The old
Q-value for the current state-action pair, which is being updated.


### Key Components of Q-learning

**Off-policy Learning**:

- Q-learning is an off-policy method, meaning the agent can learn
the optimal policy independently. Used to select actions during
training (e.g., $( \epsilon )$-greedy strategy).


### Common supporting tools 

**Environment and State Management**: 

```python
Functions get_next_state() and get_reward()
```

**Q-Table Initialization**: 

```python
q_table = np.zeros((grid_size, grid_size, 4)) 
```


### Common supporting tools 

**Neural Network**: A computational model inspired by the human
brain, consisting of layers of interconnected nodes (neurons). It
learns to recognize patterns in data by adjusting weights through
multiple training iterations.

**Deep Q-Learning (DQN)**: An algorithm that combines Q-Learning
with deep neural networks to approximate the Q-value function,
allowing the agent to choose optimal actions in complex environments
with high-dimensional state spaces.

**Policy Network**: A neural network designed to output the best
action to take in a given state. It maps input states directly to
actions, enabling the agent to decide what to do without relying on
a value function.

**Policy Gradient**: An optimization technique in reinforcement
learning where the agent learns the best policy by directly
adjusting its parameters to maximize the cumulative reward, rather
than estimating value functions.


### Common supporting tools 

**Behavior Policy**: The strategy that the agent uses to explore the
environment and collect experiences. It often includes some
randomness to encourage exploration of different actions.

**Target Policy**: The policy that the agent is trying to optimize.
In some algorithms, like Q-learning, it is used to determine the
best action to take based on learned values.

**Epsilon-Greedy Policy**: A strategy used to balance exploration
and exploitation. With a small probability (epsilon), the agent
chooses a random action to explore, and with the remaining
probability (1 - epsilon), it chooses the best-known action based
on current knowledge.




## How to use it?


### Grid

The environment is a 4×4 grid. 
The agent starts in (0,0) and aims to reach (3,3).


### Environment Setup

```{python}
## Import packages
import numpy as np
import random

## Set the random seed for reproducibility
random.seed(3255)
np.random.seed(3255)

## Define the environment
grid_size = 4
start_state = (0, 0)
goal_state = (3, 3)
obstacles = []
```

### Hyperparameters

```{python}
## Define the hyperparameters for our value function
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.2  # Exploration rate
num_episodes = 1000  # Try 1000 times
```

### Q-table Initialization

```{python}
## Initialize the Q-table
q_table = np.zeros((grid_size, grid_size, 4))  
## 4 possible actions: up, down, left, right
## The output Q-table follows thie 4 directions

## Define the action space
actions = ["up", "down", "left", "right"]
```

### State Transitions and Reward Calculation

```{python}
def get_next_state(state, action):
    i, j = state
    if action == "up":
        return (max(i - 1, 0), j)
    elif action == "down":
        return (min(i + 1, grid_size - 1), j)
    elif action == "left":
        return (i, max(j - 1, 0))
    elif action == "right":
        return (i, min(j + 1, grid_size - 1))
    
def get_reward(state):
    if state == goal_state:
        return 10
    elif state in obstacles:
        return -5
    else:
        return -1
```

Move up: decrease row index (i) by 1, but don't go out of bounds
(minimum row index is 0)
Move down: increase row index (i) by 1, but don't go out of bounds
(maximum row index is grid_size - 1)
Move left: decrease column index (j) by 1, but don't go out of bounds
(minimum column index is 0)
Move right: increase column index (j) by 1, but don't go out of
bounds (maximum column index is grid_size - 1)

### Action Selection (Epsilon-Greedy Policy)

```{python}
def choose_action(state):
    ## Epsilon-greedy action selection
    if random.uniform(0, 1) < epsilon:
        return random.choice(actions)  # Explore
    else:
        ## Exploit (choose the action with the highest Q-value)
        state_q_values = q_table[state[0], state[1], :]
        return actions[np.argmax(state_q_values)]
```

state represents the agent's current position on the grid (given as
a tuple (i, j)). And there are 4 x 4 = 16 possibilities.
The function uses this state to decide whether to explore (choose a
random action) or exploit (choose the best-known action based on the Q-table).
The Q-values for that particular state are retrieved using q_table
[state[0], state[1], :].


### Q-L Algorithm (Main Loop)

```{python}
## Q-learning Algorithm
for episode in range(num_episodes):
    state = start_state
    while state != goal_state:
        action = choose_action(state)
        next_state = get_next_state(state, action)
        
        ## Get the reward for moving to the next state
        reward = get_reward(next_state)
        
        ## Update Q-value using the Q-learning formula
        current_q_value = q_table[
            state[0], state[1], actions.index(action)
            ]
        max_future_q_value = np.max(
            q_table[next_state[0], next_state[1], :]
            )
        new_q_value = current_q_value + alpha * (
            reward + gamma * max_future_q_value - current_q_value
            )
        q_table[state[0], state[1], actions.index(action)] = new_q_value
        
        state = next_state
```

state is initially set to start_state (e.g., (0, 0)).
Within the loop, the choose_action(state) function is called to select an action based on the agent's current position.
The agent then moves to a new next_state using the chosen action, and the current state is updated to next_state.
Throughout this loop, state always represents the agent’s current position on the grid as it progresses toward the goal_state.


### Result

```{python}
## Display the learned Q-values
print("Learned Q-Table:")
print(q_table)
```

This is the directly output of this example, there are three layers
of bracket, each of them have different meanings.
First layer of brackets: Represents the rows of the grid. Each layer
represents a row in the Q-table(i.e., a row position in the qrid
environment).
Second layer of brackets: Represents the columns of the grid. Each
subarray represents a specificstate in that row (i.e., a specific
position in the qrid, such as (0,0), (1,1), etc.).
Third layer of brackets: Represents the Q-values for each action in
that state. Each elementrepresents the Q-value of a specific action
in that state (e.g. the four actions: up, down, left, right)


### route visualization

```{python}
import matplotlib.pyplot as plt

# Define directional arrows for actions: up, down, left, right
directions = {0: '↑', 1: '↓', 2: '←', 3: '→'}

# Visualization of the best actions on the grid
fig, ax = plt.subplots(figsize=(6, 6))
ax.set_xticks(np.arange(0.5, grid_size, 1))
ax.set_yticks(np.arange(0.5, grid_size, 1))
ax.grid(True, which='both')
ax.set_xticklabels([])
ax.set_yticklabels([])

# Highlight Start, Goal, and Obstacles
ax.add_patch(
    plt.Rectangle(
        start_state, 1, 1, fill=True, color='yellow', alpha=0.3
        )
    )
ax.add_patch(
    plt.Rectangle(
        goal_state, 1, 1, fill=True, color='green', alpha=0.3
        )
    )

# Highlight obstacles in red
for obstacle in obstacles:
    ax.add_patch(
        plt.Rectangle(
            obstacle, 1, 1, fill=True, color='red', alpha=0.5
            )
        )

# Add arrows (text-based) for the best actions
for i in range(grid_size):
    for j in range(grid_size):
        if (i, j) in obstacles:
            continue  # Skip drawing arrows for obstacle locations
        q_values = q_table[i, j, :]
        best_action_idx = np.argmax(q_values)
        best_action = directions[best_action_idx]  
        # Text-based direction arrows (↑, ↓, ←, →)

        # Adding text-based arrows at the grid cells
        ax.text(
            j + 0.5, i + 0.5, best_action, ha='center', va='center', fontsize=20
            )

# Draw grid lines and adjust axis
plt.xlim(0, grid_size)
plt.ylim(0, grid_size)
plt.gca().invert_yaxis()  
# Invert Y-axis to display it in the correct orientation
plt.show()
```


### Grid with obstacles

Because of the reusing and stating cause the output become exactly
the same. I rewrote the example with obstacles.
```{python}
import numpy as np
import matplotlib.pyplot as plt

## Set random seed for reproducibility
random.seed(3255)
np.random.seed(3255)

## Define the environment
grid_size = 4
start_state = (0, 0)
goal_state = (3, 3)
obstacles = [(1, 1), (2, 2)]  # Ensure obstacles are unique

## Define directions for visualization
directions = {0: '↑', 1: '↓', 2: '←', 3: '→'}
```

### Run the grid with obstacles

```{python}
## Function to get reward
def get_reward(state):
    if state == goal_state:
        return 10
    elif state in obstacles:
        return -5
    else:
        return -1

## Function to get the next state based on the action
def get_next_state(state, action):
    i, j = state
    if action == "up":
        next_state = (max(i - 1, 0), j)
    elif action == "down":
        next_state = (min(i + 1, grid_size - 1), j)
    elif action == "left":
        next_state = (i, max(j - 1, 0))
    elif action == "right":
        next_state = (i, min(j + 1, grid_size - 1))
    
    ## Prevent moving into obstacles
    if next_state in obstacles:
        return state  
        # Stay in the same position if the next state is an obstacle
    else:
        return next_state

## Visualization function
def plot_grid_with_obstacles():
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.clear()  # Clear the plot to avoid overlap
    ax.set_xticks(np.arange(0.5, grid_size, 1))
    ax.set_yticks(np.arange(0.5, grid_size, 1))
    ax.grid(True, which='both')
    ax.set_xticklabels([])
    ax.set_yticklabels([])

    ## Highlight Start, Goal, and Obstacles
    ax.add_patch(
        plt.Rectangle(
            start_state, 1, 1, fill=True, color='yellow', alpha=0.3
            )
        )
    ax.add_patch(
        plt.Rectangle(
            goal_state, 1, 1, fill=True, color='green', alpha=0.3
            )
        )

    ## Highlight obstacles in red
    for obstacle in set(obstacles):  # Use a set to ensure uniqueness
        ax.add_patch(
            plt.Rectangle(
                obstacle, 1, 1, fill=True, color='red', alpha=0.5
                )
            )

    ## Add arrows for the best actions from Q-table
    for i in range(grid_size):
        for j in range(grid_size):
            if (i, j) in obstacles:
                continue  # Skip arrows for obstacle locations
            q_values = q_table[i, j, :]
            best_action_idx = np.argmax(q_values)
            best_action = directions[best_action_idx]  
            # Directional arrows (↑, ↓, ←, →)
            ax.text(
                j + 0.5, i + 0.5, best_action, ha='center', va='center', fontsize=20
                )

    ## Draw grid lines and adjust axis
    plt.xlim(0, grid_size)
    plt.ylim(0, grid_size)
    plt.gca().invert_yaxis()  
    # Invert Y-axis for correct orientation
    plt.show()

## Call the visualization function
plot_grid_with_obstacles()
```




### Rewards and Penalties

```python
## Initialize list to store cumulative rewards for each episode
cumulative_rewards = []

for episode in range(num_episodes):
    state = start_state
    episode_reward = 0  # Track total reward for the current episode
    
    while state != goal_state:
        action = choose_action(state)
        next_state = get_next_state(state, action)
        
        reward = get_reward(next_state)
        episode_reward += reward  # Accumulate reward for this episode
        
        # Update Q-value
        current_q_value = q_table[
            state[0], state[1], actions.index(action)
            ]
        max_future_q_value = np.max(
            q_table[next_state[0], next_state[1], :]
            )
        new_q_value = current_q_value + alpha * 
                        (
                            reward + gamma * max_future_q_value - current_q_value
                            )
        q_table[state[0], state[1], actions.index(action)] = new_q_value
        
        state = next_state  # Move to the next state
    
    cumulative_rewards.append(episode_reward)  
    # Store cumulative reward for this episode

## Visualization of Cumulative Rewards
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(range(num_episodes), cumulative_rewards, 
            label='Cumulative Reward per Episode')
plt.xlabel("Episode")
plt.ylabel("Cumulative Reward")
plt.title("Cumulative Rewards and Penalties Over Episodes")
plt.legend()
plt.show()
```


### Track Cumulative Rewards Over Episodes

```{python}
## Initialize list to store cumulative rewards for each episode
cumulative_rewards = []

for episode in range(num_episodes):
    state = start_state
    episode_reward = 0  # Track total reward for the current episode
    
    while state != goal_state:
        action = choose_action(state)
        next_state = get_next_state(state, action)
        
        reward = get_reward(next_state)
        episode_reward += reward  # Accumulate reward for this episode
        
        # Update Q-value
        current_q_value = q_table[
            state[0], state[1], actions.index(action)
            ]
        max_future_q_value = np.max(
            q_table[next_state[0], next_state[1], :]
            )
        new_q_value = current_q_value + alpha * (
            reward + gamma * max_future_q_value - current_q_value
            )
        q_table[state[0], state[1], actions.index(action)] = new_q_value
        
        state = next_state  # Move to the next state
    
    cumulative_rewards.append(episode_reward)  
    # Store cumulative reward for this episode

## Visualization of Cumulative Rewards
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(
    range(num_episodes), cumulative_rewards, label='Cumulative Reward per Episode'
    )
plt.xlabel("Episode")
plt.ylabel("Cumulative Reward")
plt.title("Cumulative Rewards and Penalties Over Episodes")
plt.legend()
plt.show()
```




## Demonstration: Tic-Tac-Toe 

In this demonstration, we'll develop a reinforcement learning agent
that learns to play Tic-Tac-Toe using the Q-learning algorithm. We'll
start with an overview of the work plan and then present the code step
by step, explaining each part in detail.



### Work Plan Overview

1. **Import Required Libraries**: Import necessary Python libraries for the implementation.

2. **Define the Default Q-Value Function**: Create a function to initialize default Q-values for unseen states.

3. **Implement the `TicTacToe` Game Class**: Define the game environment, including the board, moves, and win conditions.

4. **Implement the `QLearningAgent` Class**: Develop the agent that will learn optimal strategies using Q-learning.

5. **Define the Game Playing Function**: Write a function to simulate games between the agent and an opponent.

6. **Define the Training Function**: Create a function to train the agent over multiple episodes.

7. **Define the Evaluation Function**: Assess the agent's performance after training.

8. **Enable Human Interaction**: Allow a human player to play against the trained agent.

9. **Main Function**: Tie all components together and provide a user interface.

### Code Implementation

Let's go through the code step by step.

#### Import Required Libraries

We start by importing the necessary libraries.

```{python}
import numpy as np
import random
from collections import defaultdict
```

#### Define the Default Q-Value Function

We define a function that returns a NumPy array of zeros, which
initializes the Q-values for new states.

```{python}
def default_q_value():
    return np.zeros(9)
```

This function ensures that every new state encountered by the agent
has an initial Q-value of zero for all possible actions.


#### Implement the `TicTacToe` Game Class

We create a class to represent the Tic-Tac-Toe game environment.
```{python}
class TicTacToe:
    def __init__(self):
        self.board = [' '] * 9
        self.current_winner = None

    def reset(self):
        self.board = [' '] * 9
        self.current_winner = None
        return self.get_state()

    def available_actions(self):
        return [i for i, spot in enumerate(self.board) if spot == ' ']

    def get_state(self):
        return tuple(self.board)

    def make_move(self, square, letter):
        if self.board[square] == ' ':
            self.board[square] = letter
            if self.winner(square, letter):
                self.current_winner = letter
            return True
        return False

    def winner(self, square, letter):
        # Check rows, columns, and diagonals for a win
        row_ind = square // 3
        row = self.board[row_ind*3:(row_ind+1)*3]
        if all(s == letter for s in row):
            return True

        col_ind = square % 3
        col = [self.board[col_ind+i*3] for i in range(3)]
        if all(s == letter for s in col):
            return True

        # Check diagonals
        if square % 2 == 0:
            diag1 = [self.board[i] for i in [0,4,8]]
            if all(s == letter for s in diag1):
                return True
            diag2 = [self.board[i] for i in [2,4,6]]
            if all(s == letter for s in diag2):
                return True

        return False

    def is_full(self):
        return ' ' not in self.board

    def print_board(self):
        # Helper function to print the board
        for row in [self.board[i*3:(i+1)*3] for i in range(3)]:
            print('| ' + ' | '.join(row) + ' |')

    def print_board_nums(self):
        # Helper function to show the number mapping to board positions
        number_board = [str(i) for i in range(9)]
        for row in [number_board[i*3:(i+1)*3] for i in range(3)]:
            print('| ' + ' | '.join(row) + ' |')
```


Explanation:

+ `__init__`: Initializes the game board and sets the current winner to `None`.
+ `reset`: Resets the board for a new game and returns the initial state.
+ `available_actions`: Returns a list of indices where moves can be
  made.
+ `get_state`: Returns a tuple representing the current state of the
  board.
+ `make_move`: Places a letter ('X' or 'O') on the board if the move
  is valid.
+ `winner`: Checks if the last move resulted in a win.
+ `is_full`: Checks if the board is full, indicating a draw.
+ `print_board` and `print_board_nums`: Helper methods to display the
  board and the numbering for positions.
  
  
#### Implement the QLearningAgent Class

We define a class for the agent that will learn using Q-learning.
```{python}
class QLearningAgent:
    def __init__(self, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.9995):
        self.q_table = defaultdict(default_q_value)
        self.alpha = alpha          # Learning rate
        self.gamma = gamma          # Discount factor
        self.epsilon = epsilon      # Exploration rate
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.01     # Minimum exploration rate

    def choose_action(self, state, available_actions):
        # ε-greedy action selection
        if np.random.rand() < self.epsilon:
            return random.choice(available_actions)
        else:
            state_values = self.q_table[state]
            # Select action with highest Q-value among available actions
            q_values = [(action, state_values[action]) for action in available_actions]
            max_value = max(q_values, key=lambda x: x[1])[1]
            max_actions = [action for action, value in q_values if value == max_value]
            return random.choice(max_actions)

    def learn(self, state, action, reward, next_state, done):
        old_value = self.q_table[state][action]
        next_max = np.max(self.q_table[next_state]) if not done else 0
        # Q-learning update rule
        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)
        self.q_table[state][action] = new_value

        # Decay the exploration rate
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

Explanation:

+ `__init__`: Initializes the Q-table and sets the hyperparameters for
  learning.
+ `choose_action`: Implements the ε-greedy policy for choosing
  actions.
    -  With probability $\epsilon$, the agent explores by selecting a
       random action.
    - Otherwise, it exploits by choosing the action with the highest
      estimated Q-value.
+ `learn`: Updates the Q-values based on the reward received and the
  maximum Q-value of the next state.


#### Define the Game Playing Function

We create a function to simulate a game between the agent and an
opponent.

```{python}
def play_game(agent, env, human_vs_agent=False):
    state = env.reset()
    done = False
    if human_vs_agent:
        print("Positions are as follows:")
        env.print_board_nums()
    current_player = 'X'  # Agent always plays 'X'

    while not done:
        if current_player == 'X':
            available_actions = env.available_actions()
            action = agent.choose_action(state, available_actions)
            env.make_move(action, 'X')
            next_state = env.get_state()
            if human_vs_agent:
                print("\nAgent's Move:")
                env.print_board()
            if env.current_winner == 'X':
                agent.learn(state, action, 1, next_state, True)
                if human_vs_agent:
                    print("Agent wins!")
                return 1  # Agent wins
            elif env.is_full():
                agent.learn(state, action, 0.5, next_state, True)
                if human_vs_agent:
                    print("It's a draw.")
                return 0.5  # Draw
            else:
                agent.learn(state, action, 0, next_state, False)
                state = next_state
                current_player = 'O'
        else:
            available_actions = env.available_actions()
            if human_vs_agent:
                valid_square = False
                while not valid_square:
                    user_input = input("Your move (0-8): ")
                    try:
                        action = int(user_input)
                        if action not in available_actions:
                            raise ValueError
                        valid_square = True
                    except ValueError:
                        print("Invalid move. Try again.")
                env.make_move(action, 'O')
                state = env.get_state()
            else:
                action = random.choice(available_actions)
                env.make_move(action, 'O')
            if env.current_winner == 'O':
                agent.learn(state, action, -1, env.get_state(), True)
                if human_vs_agent:
                    env.print_board()
                    print("You win!")
                return -1  # Agent loses
            elif env.is_full():
                agent.learn(state, action, 0.5, env.get_state(), True)
                if human_vs_agent:
                    print("It's a draw.")
                return 0.5  # Draw
            else:
                current_player = 'X'
```

Explanation:

+ Game Loop: Alternates turns between the agent and the opponent (or
  human player).
+ Agent's Turn:
    - Chooses an action using the ε-greedy policy.
	- Updates the Q-table based on the outcome.
+ Opponent's/Human's Turn:
    - If `human_vs_agent` is `True`, prompts the human for input.
	- Otherwise, the opponent makes a random move.
	- The agent updates its Q-table based on the outcome.
	
#### Define the Training Function

We define a function to train the agent over multiple episodes.

```{python}
def train_agent(episodes=50000):
    agent = QLearningAgent()
    env = TicTacToe()
    for episode in range(episodes):
        play_game(agent, env)
        if (episode + 1) % 10000 == 0:
            print(f"Episode {episode + 1}/{episodes} completed.")
    return agent
```

Explanation:

+ Initialization: Creates a new agent and game environment.
+ Training Loop: The agent plays the game repeatedly to learn from
  experience.
+ Progress Updates: Prints a message every 10,000 episodes to track
  training progress.


#### Define the Evaluation Function
We create a function to evaluate the agent's performance after
training.
```{python}
def evaluate_agent(agent, games=1000):
    env = TicTacToe()
    wins = 0
    draws = 0
    losses = 0
    for _ in range(games):
        result = play_game(agent, env)
        if result == 1:
            wins += 1
        elif result == 0.5:
            draws += 1
        else:
            losses += 1
    print(f"Out of {games} games: {wins} wins, {draws} draws, {losses} losses.")
```

Explanation:

+ Evaluation Loop: The agent plays a specified number of games without
  learning.
+ Outcome Tracking: Records the number of wins, draws, and losses.
+ Performance Display: Prints the results after evaluation.

#### Enable Human Interaction
We create a function to allow a human to play against the agent.

```{python}
def play_against_agent(agent):
    env = TicTacToe()
    play_game(agent, env, human_vs_agent=True)
```

#### Main Function
We define the main function to provide a user interface.
```{python}
def main():
    print("Tic-Tac-Toe with Reinforcement Learning Agent")
    print("1. Train Agent")
    print("2. Evaluate Agent")
    print("3. Play Against Agent")
    choice = input("Select an option (1-3): ")

    if choice == '1':
        episodes = int(input("Enter number of training episodes: "))
        agent = train_agent(episodes)
        # Save the trained agent
        import pickle
        with open('trained_agent.pkl', 'wb') as f:
            pickle.dump(agent, f)
        print("Agent trained and saved as 'trained_agent.pkl'.")
    elif choice == '2':
        # Load the trained agent
        import pickle
        try:
            with open('trained_agent.pkl', 'rb') as f:
                agent = pickle.load(f)
            evaluate_agent(agent)
        except FileNotFoundError:
            print("No trained agent found. Please train the agent first.")
    elif choice == '3':
        # Load the trained agent
        import pickle
        try:
            with open('trained_agent.pkl', 'rb') as f:
                agent = pickle.load(f)
            play_against_agent(agent)
        except FileNotFoundError:
            print("No trained agent found. Please train the agent first.")
    else:
        print("Invalid option selected.")
```


#### Example Session

Training and Evaluating the Agent:
```{pythopn}
agent = train_agent(50000)

evaluate_agent(agent, games=1000)
```

Playing Against the Agent:
```{python}
# play_against_agent(agent)
```


